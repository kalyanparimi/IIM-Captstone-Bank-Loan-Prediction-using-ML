{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 855,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import VarianceThreshold, RFE, SelectFromModel, SelectKBest, f_classif, chi2, mutual_info_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# motivation of feature selection\n",
    "# Adding more features make the model more complex, and the model may overfit the data. By removing the unimportant features,\n",
    "# the model may generalize better. \n",
    "# this totorial is based on the material on sklearn website and other resources. Please see the reference at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 904,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data, for more information of this data: https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 905,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]]\n",
      "(150, 4)\n"
     ]
    }
   ],
   "source": [
    "print(X[: 5, :])\n",
    "print(X.shape)\n",
    "# the data have 4 features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 858,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 14)"
      ]
     },
     "execution_count": 858,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In order to test the effectness of different feature selection methods, we add some noise features to the dataset.\n",
    "np.random.seed(100)\n",
    "E = np.random.uniform(0, 1, size=(len(X), 10))\n",
    "X = np.hstack((X, E))\n",
    "X.shape\n",
    "# now the data have 14 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 859,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105, 14)"
      ]
     },
     "execution_count": 859,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Before applying feature selection method, we need to split the data first.\n",
    "# The reason for this is that we only select features based on the information on the training set, not on the whole data set. \n",
    "# We should keep part of the whole data set as test set to evaluate the performance of the feature selection and the model.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=100, test_size=0.3)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing features with low variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VarianceThreshold is a simple baseline approach to feature selection. It removes all features whose variance\n",
    "# doesnâ€™t meet some threshold. By default, it removes all zero-variance features, \n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html#sklearn.feature_selection.VarianceThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 860,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105, 14)"
      ]
     },
     "execution_count": 860,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sel_variance_threshold = VarianceThreshold() \n",
    "X_train_remove_variance = sel_variance_threshold.fit_transform(X_train)\n",
    "X_train_remove_variance.shape\n",
    "# The data still has 14 features, none of the features were removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariate feature selection works by selecting the best features based on univariate statistical tests.\n",
    "# we compare each feature to the target variable, to see whether there is statistially significant relationship between them.\n",
    "# It is also called analysis of variance (ANOVA). \n",
    "# When we analyze the relationship between one feature and the target variable  \n",
    "# we ignore the other features. That is why it is called 'univariate'.\n",
    "# Each feature has its own test score. \n",
    "# Finally, all the test scores are compared, and the features with top scores will be selected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 861,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True  True False False False False False False False False\n",
      " False False]\n",
      "{'k': 4, 'score_func': <function chi2 at 0x0000022EBF1BE1E0>}\n"
     ]
    }
   ],
   "source": [
    "# (1) use chi square test. For more information about chi square test, read:\n",
    "# a.  statistics: http://vassarstats.net/textbook/  , chaper 8\n",
    "# b.  sklearn: https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.chi2.html#sklearn.feature_selection.chi2\n",
    "# read the source code if you want to know more about how sklearn apply chi squre test: https://github.com/scikit-learn/scikit-learn/blob/1495f6924/sklearn/feature_selection/univariate_selection.py#L172\n",
    "\n",
    "sel_chi2 = SelectKBest(chi2, k=4) # select 4 features\n",
    "X_train_chi2 = sel_chi2.fit_transform(X_train, y_train)\n",
    "\n",
    "print(sel_chi2.get_support())\n",
    "print(sel_chi2.get_params())\n",
    "\n",
    "# The first 4 elements in the array are trues, which means the first 4 features were selected by this method.\n",
    "# Since the first 4 features are the original features in the data, the chi squre test performs well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 862,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True  True False False False False False False False False\n",
      " False False]\n",
      "{'k': 4, 'score_func': <function f_classif at 0x0000022EBF1BE0D0>}\n"
     ]
    }
   ],
   "source": [
    "# (2) use f test \n",
    "# a. statistics\n",
    "#   one-way ANOVA : http://vassarstats.net/textbook/  , chaper 14\n",
    "#   ANOVA for regression: http://facweb.cs.depaul.edu/sjost/csc423/documents/f-test-reg.htm\n",
    "# b.  sklearn\n",
    "# f test for classification: https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_classif.html#sklearn.feature_selection.f_classif\n",
    "# f test for regression: https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_regression.html#sklearn.feature_selection.f_regression\n",
    "\n",
    "sel_f = SelectKBest(f_classif, k=4)\n",
    "X_train_f = sel_f.fit_transform(X_train, y_train)\n",
    "\n",
    "print(sel_f.get_support())\n",
    "print(sel_f.get_params())\n",
    "\n",
    "# The f test can correctly select the original features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 863,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True  True False False False False False False False False\n",
      " False False]\n",
      "{'k': 4, 'score_func': <function mutual_info_classif at 0x0000022EBF280730>}\n"
     ]
    }
   ],
   "source": [
    "# (2) use mutual_info_classif test \n",
    "# sklearn\n",
    "# for classification: https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html#sklearn.feature_selection.mutual_info_classif\n",
    "# for regression: https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_regression.html#sklearn.feature_selection.mutual_info_regression\n",
    "\n",
    "sel_mutual = SelectKBest(mutual_info_classif, k=4)\n",
    "X_train_mutual = sel_mutual.fit_transform(X_train, y_train)\n",
    "\n",
    "print(sel_mutual.get_support())\n",
    "print(sel_mutual.get_params())\n",
    "\n",
    "# The mutual_info_classif test can correctly select the original features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In sum, three tests produce the same result. \n",
    "# We use the iris data as a classification problem. For regression probelm, similarly, we can use f_regression, mutual_info_regression \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive feature elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), \n",
    "# recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. \n",
    "# First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through \n",
    "#a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current \n",
    "# set of features.That procedure is recursively repeated on the pruned set until the desired number of features to select \n",
    "# is eventually reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 898,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the logistic regresssion as the model\n",
    "# about RFE in sklearn: https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html#sklearn.feature_selection.RFE\n",
    "\n",
    "model_logistic = LogisticRegression(solver='lbfgs', multi_class='multinomial', max_iter=1000)\n",
    "sel_rfe_logistic = RFE(estimator=model_logistic, n_features_to_select=4, step=1)\n",
    "X_train_rfe_logistic = sel_rfe_logistic.fit_transform(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 899,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False  True  True  True False False False False False False False False\n",
      " False  True]\n",
      "{'estimator__C': 1.0, 'estimator__class_weight': None, 'estimator__dual': False, 'estimator__fit_intercept': True, 'estimator__intercept_scaling': 1, 'estimator__l1_ratio': None, 'estimator__max_iter': 1000, 'estimator__multi_class': 'multinomial', 'estimator__n_jobs': None, 'estimator__penalty': 'l2', 'estimator__random_state': None, 'estimator__solver': 'lbfgs', 'estimator__tol': 0.0001, 'estimator__verbose': 0, 'estimator__warm_start': False, 'estimator': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
      "                   multi_class='multinomial', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False), 'n_features_to_select': 4, 'step': 1, 'verbose': 0}\n"
     ]
    }
   ],
   "source": [
    "print(sel_rfe_logistic.get_support())\n",
    "print(sel_rfe_logistic.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 895,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3,  1,  1,  1,  9,  8,  7,  6,  5,  4,  2, 11, 10,  1])"
      ]
     },
     "execution_count": 895,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sel_rfe_logistic.ranking_\n",
    "# The features were selected will all be rank 1.\n",
    "\n",
    "# The recursive feature elimination select the only part of the original features and 1 noise feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 896,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets try another model: the random forest\n",
    "\n",
    "model_tree = RandomForestClassifier(random_state=100, n_estimators=50)\n",
    "sel_rfe_tree = RFE(estimator=model_tree, n_features_to_select=4, step=1)\n",
    "X_train_rfe_tree = sel_rfe_tree.fit_transform(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 900,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True  True False False False False False False False False\n",
      " False False]\n",
      "{'estimator__bootstrap': True, 'estimator__class_weight': None, 'estimator__criterion': 'gini', 'estimator__max_depth': None, 'estimator__max_features': 'auto', 'estimator__max_leaf_nodes': None, 'estimator__min_impurity_decrease': 0.0, 'estimator__min_impurity_split': None, 'estimator__min_samples_leaf': 1, 'estimator__min_samples_split': 2, 'estimator__min_weight_fraction_leaf': 0.0, 'estimator__n_estimators': 50, 'estimator__n_jobs': None, 'estimator__oob_score': False, 'estimator__random_state': 100, 'estimator__verbose': 0, 'estimator__warm_start': False, 'estimator': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=50,\n",
      "                       n_jobs=None, oob_score=False, random_state=100,\n",
      "                       verbose=0, warm_start=False), 'n_features_to_select': 4, 'step': 1, 'verbose': 0}\n"
     ]
    }
   ],
   "source": [
    "print(sel_rfe_tree.get_support())\n",
    "print(sel_rfe_tree.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using randome foreast as the model in the recursive feature selection can select the right features in this case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection using SelectFromModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SelectFromModel is a meta-transformer that can be used along with any estimator that has a coef_ or feature_importances_ \n",
    "# attribute after fitting. The features are considered unimportant and removed, \n",
    "# if the corresponding coef_ or feature_importances_ values are below the provided threshold parameter. \n",
    "# compared to univariate feature selection, model-based feature selection consider all feature at once, thus can capture interactions\n",
    "# the model used for the feature selection dosen't need to be the same model for traning later.\n",
    "# sklearn: https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html#sklearn.feature_selection.SelectFromModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) L1-based feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear model with L1 penalty can eliminate some of the features, \n",
    "# thus can act as feature selection method before using another model to fit the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 867,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_logistic = LogisticRegression(solver='saga', multi_class='multinomial', max_iter=10000, penalty='l1')\n",
    "sel_model_logistic = SelectFromModel(estimator=model_logistic)\n",
    "X_train_sfm_l1 = sel_model_logistic.fit_transform(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 868,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True False  True  True False False False False False  True False False\n",
      " False False]\n",
      "{'estimator__C': 1.0, 'estimator__class_weight': None, 'estimator__dual': False, 'estimator__fit_intercept': True, 'estimator__intercept_scaling': 1, 'estimator__l1_ratio': None, 'estimator__max_iter': 10000, 'estimator__multi_class': 'multinomial', 'estimator__n_jobs': None, 'estimator__penalty': 'l1', 'estimator__random_state': None, 'estimator__solver': 'saga', 'estimator__tol': 0.0001, 'estimator__verbose': 0, 'estimator__warm_start': False, 'estimator': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=10000,\n",
      "                   multi_class='multinomial', n_jobs=None, penalty='l1',\n",
      "                   random_state=None, solver='saga', tol=0.0001, verbose=0,\n",
      "                   warm_start=False), 'max_features': None, 'norm_order': 1, 'prefit': False, 'threshold': None}\n"
     ]
    }
   ],
   "source": [
    "print(sel_model_logistic.get_support())\n",
    "print(sel_model_logistic.get_params())\n",
    "\n",
    "# The SelectFromModel select the wrong features. Feature 2 in the original dataset was ignored\n",
    "# while one feature from the noice was selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2)  Tree-based feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 891,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.11844633 0.09434048 0.21340848 0.33708242 0.02019553 0.03081254\n",
      " 0.02317242 0.01962394 0.02407251 0.02193159 0.03289007 0.01836624\n",
      " 0.01811144 0.027546  ]\n"
     ]
    }
   ],
   "source": [
    "# let's try another model\n",
    "model_tree = RandomForestClassifier(random_state=100, n_estimators=50)\n",
    "model_tree.fit(X_train, y_train)\n",
    "print(model_tree.feature_importances_)\n",
    "\n",
    "sel_model_tree = SelectFromModel(estimator=model_tree, prefit=True, threshold='mean')  \n",
    "                    # since we already fit the data, we specify prefit option here\n",
    "                    # Features whose importance is greater or equal to the threshold are kept while the others are discarded.\n",
    "X_train_sfm_tree = sel_model_tree.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 892,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True  True False False False False False False False False\n",
      " False False]\n",
      "{'estimator__bootstrap': True, 'estimator__class_weight': None, 'estimator__criterion': 'gini', 'estimator__max_depth': None, 'estimator__max_features': 'auto', 'estimator__max_leaf_nodes': None, 'estimator__min_impurity_decrease': 0.0, 'estimator__min_impurity_split': None, 'estimator__min_samples_leaf': 1, 'estimator__min_samples_split': 2, 'estimator__min_weight_fraction_leaf': 0.0, 'estimator__n_estimators': 50, 'estimator__n_jobs': None, 'estimator__oob_score': False, 'estimator__random_state': 100, 'estimator__verbose': 0, 'estimator__warm_start': False, 'estimator': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=50,\n",
      "                       n_jobs=None, oob_score=False, random_state=100,\n",
      "                       verbose=0, warm_start=False), 'max_features': None, 'norm_order': 1, 'prefit': True, 'threshold': 'mean'}\n"
     ]
    }
   ],
   "source": [
    "print(sel_model_tree.get_support())\n",
    "print(sel_model_tree.get_params())\n",
    "\n",
    "# Based on the tree model, the SelectFromModel select the correct features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 871,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In summary: for this particular data set,  While using logistic model as recursive feature elimination or model selection, \n",
    "# it select the features incorretly. On the other hand, all other feature selection methods \n",
    "# select the first four features correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's compare the performance before and after the feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  (1) Before feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 872,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=10000,\n",
       "                   multi_class='multinomial', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='saga', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 872,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_logistic = LogisticRegression(solver='saga', multi_class='multinomial', max_iter=10000)\n",
    "model_logistic.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 873,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16  0  0]\n",
      " [ 0 11  0]\n",
      " [ 0  2 16]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        16\n",
      "           1       0.85      1.00      0.92        11\n",
      "           2       1.00      0.89      0.94        18\n",
      "\n",
      "    accuracy                           0.96        45\n",
      "   macro avg       0.95      0.96      0.95        45\n",
      "weighted avg       0.96      0.96      0.96        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict = model_logistic.predict(X_test)\n",
    "print(confusion_matrix(y_test, predict))\n",
    "print(classification_report(y_test, predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) After feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 874,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=10000,\n",
       "                   multi_class='multinomial', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='saga', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 874,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we use the result from the feature selection based on the chi squre test\n",
    "# X_train_chi2 is the data after the feature selection to feed into the model\n",
    "\n",
    "model_logistic = LogisticRegression(solver='saga', multi_class='multinomial', max_iter=10000)\n",
    "model_logistic.fit(X_train_chi2, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 880,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45, 14)\n",
      "(45, 4)\n"
     ]
    }
   ],
   "source": [
    "# We also need to transform the test data because the number of features were changed\n",
    "\n",
    "X_test_chi2 = sel_chi2.transform(X_test)\n",
    "print(X_test.shape)\n",
    "print(X_test_chi2.shape)\n",
    "\n",
    "# Only use the features in the test set that are corresponding to the remaining features in the training set. 4 features in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 881,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16  0  0]\n",
      " [ 0 11  0]\n",
      " [ 0  1 17]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        16\n",
      "           1       0.92      1.00      0.96        11\n",
      "           2       1.00      0.94      0.97        18\n",
      "\n",
      "    accuracy                           0.98        45\n",
      "   macro avg       0.97      0.98      0.98        45\n",
      "weighted avg       0.98      0.98      0.98        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict = model_logistic.predict(X_test_chi2)\n",
    "print(confusion_matrix(y_test, predict))\n",
    "print(classification_report(y_test, predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 877,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In sum, feature selection remove the noice and generalize the model better, thus improve the model performance\n",
    "\n",
    "E = np.random.uniform(0, 1, size=(len(X), 10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 879,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<tokenize>\"\u001b[1;36m, line \u001b[1;32m8\u001b[0m\n\u001b[1;33m    0       1.00      1.00      1.00        16\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "Before\n",
    "\n",
    "[[16  0  0]\n",
    " [ 0 11  0]\n",
    " [ 0  2 16]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       1.00      1.00      1.00        16\n",
    "           1       0.85      1.00      0.92        11\n",
    "           2       1.00      0.89      0.94        18\n",
    "\n",
    "    accuracy                           0.96        45\n",
    "   macro avg       0.95      0.96      0.95        45\n",
    "weighted avg       0.96      0.96      0.96        45\n",
    "\n",
    "After \n",
    "[[16  0  0]\n",
    " [ 0 11  0]\n",
    " [ 0  1 17]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       1.00      1.00      1.00        16\n",
    "           1       0.92      1.00      0.96        11\n",
    "           2       1.00      0.94      0.97        18\n",
    "\n",
    "    accuracy                           0.98        45\n",
    "   macro avg       0.97      0.98      0.98        45\n",
    "weighted avg       0.98      0.98      0.98        45\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can experiment on adding different noise. for example: E = np.random.uniform(0, 10, size=(len(X), 10))\n",
    "# if the number of features in the noice increase to 20 and the noice is bigger, then the feature selection might select\n",
    "# the wrong features. However, the performance is still improved!\n",
    "\n",
    "E = np.random.uniform(0, 10, size=(len(X), 20))\n",
    "\n",
    "Before \n",
    "\n",
    "[[16  0  0]\n",
    " [ 0 10  1]\n",
    " [ 0  4 14]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       1.00      1.00      1.00        16\n",
    "           1       0.71      0.91      0.80        11\n",
    "           2       0.93      0.78      0.85        18\n",
    "\n",
    "    accuracy                           0.89        45\n",
    "   macro avg       0.88      0.90      0.88        45\n",
    "weighted avg       0.90      0.89      0.89        45\n",
    "\n",
    "After \n",
    "[[16  0  0]\n",
    " [ 0 10  1]\n",
    " [ 0  1 17]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       1.00      1.00      1.00        16\n",
    "           1       0.91      0.91      0.91        11\n",
    "           2       0.94      0.94      0.94        18\n",
    "\n",
    "    accuracy                           0.96        45\n",
    "   macro avg       0.95      0.95      0.95        45\n",
    "weighted avg       0.96      0.96      0.96        45"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
